--- 
title: "Minería de datos"
subtitle: "Aplicaciones en R con bases de datos de Bolivia"
author: "MSc. Alvaro Limber Chirino Gutierrez"
description: "Este texto esta destinado a estudiantes de la carrera de Estadística de la Universidad Mayor de San Andres."
date: "`r Sys.Date()`"
delete_merged_file: true
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
graphics: yes
github-repo: alvarolimber/MD_umsa
bibliography: book.bib
nocite: '@*'
pandoc_args: ["-Fpandoc-crossref"]
---
# Prefacio {-}

```{r fig.align='center', echo=FALSE, include=identical(knitr:::pandoc_to(), 'html'), fig.link='https://www.crcpress.com/product/isbn/9781138700109'}
knitr::include_graphics('images/cover.png', dpi = NA)
```

<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a><br />Este documento de [Alvaro Chirino](https://twiiter/alvarochirinog) esta bajo la licencia de <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.

## Audiencia  {-}

El libro fue diseñado originalmente para estudiantes de la carrera de Estadística de la Universidad Mayor de San Andrés.

Este documento representa un primer acercamiento a los estudiantes de estadística  al software R y al mundo de la minería de datos.

## Estructura del libro  {-}

El libro incluye los siguientes capítulos:

1. Introducción a la minería de datos
2. Pre procesamiento

## Software y acuerdos {-}

```{r}
sessionInfo()
```

## Datos {-}


## Agradecimiento  {-}

Peeta...

# Acerca del autor {-}

## Bibliografía {-}


<!--chapter:end:index.Rmd-->

# Minería de Datos

## Motivación para la Minería de datos

  * (Variedad) Los métodos de recolección de datos han evolucionado muy rápidamente.
  * (Volumen) Las bases de datos han crecido exponencialmente
  * (Usuarios) Estos datos contienen información útil para las empresas, países, etc.
  * (Tecnología) El tamaño hace que la inspección manual sea casi imposible
  * (Método) Se requieren métodos de análisis de datos automáticos para optimizar el uso de estos enormes conjuntos de datos

## ¿Qué es la minería de datos?

Es el análisis de **conjuntos de datos** (a menudo grandes) para encontrar **relaciones insospechadas** (conocimiento) y resumir los datos de **formas novedosas** que sean **comprensibles y útiles** para el propietario/usuario de los datos.

<p style='text-align: right;'> Principles of Data Mining (Hand et.al. 2001) </p>

## Datos y conocimiento (Insumo/Resultado)

### Datos:

  * se refieren a instancias únicas y primitivas (single objetos, personas, eventos, puntos en el tiempo, etc.)
  * describir propiedades individuales
  * a menudo son fáciles de recolectar u obtener (por ejemplo, cajeros de escáner, internet, etc.)
  * no nos permiten hacer predicciones o pronósticos

Ejercicio:

Elija un sector/área y describa los potenciales datos que se tienen disponibles

  - (R) Salud (Hospital): Registros de pacientes, medicamentos, vacunas, fichas epidemiológicas, personal médico, consultorios (infraestructura), financiera
  - (R) (Seguridad) Feminicidios: Casos, lugar, entorno familiar, denuncias previas, sospechosos, información forense, Caracterizar al culpable  
  - (A) Banca: Prestamos; ingresos, laboral. Caja de ahorro; personal, movimientos. Cajas; Servicios, etc. Cajeros; Lugar, movimientos. Clientes; Prestamistas; Deuda, mora, juicio, ahorristas. Base de datos de patrimonio 
  - (B) Geografía: Clima; Lluvia, Minas; tipo, trabajadores, cultivos; tipo, temporada, tipo suelo, montañas, pluvial, temperatura. 

### Conocimiento:

  * (Características) se refiere a clases de instancias (conjuntos de ...)
  * (Forma) describe patrones generales, estructuras, leyes,
  * (Declaración) consta de la menor cantidad de declaraciones posibles
  * (Proceso) a menudo es difícil y lleva mucho tiempo encontrar u obtener
  * (Acciones) nos permite hacer predicciones y pronósticos

Ejercicio:

  * Agrupar la tipología de los pacientes; Mejorar servicio, acciones
  * Determinantes del feminicidio; SLIM, medidas 
  * Relación el monto del ingreso con el motivo de préstamo; Realizar ofertas, promocionar a clientes.
  * Relación entre el clima y los cultivos, dado el tipo de mineral se puede pensar en un mercado posterior

## Requerimientos

  * Disponibilidad para aprender 
  * Mucha paciencia
    - Interactúa con otras áreas
    - Preprocesamiento de datos 
  * Creatividad
  * Rigor, prueba y error

## knowledge discovery in databases (KDD)

![](images/f1.png)

## Preparación de los datos

### Recopilación

* Instituto de Estadística 
* UDAPE, ASFI 
* Ministerio Salud (SNIS), Ministerio de educación (SIE)
* APIs, Twitter, Facebook, etc.
* Kaggle
* Banco Mundial, UNICEF, FAO, BID (Open Data)

### Data Warehouse

![](images/dataware.png)

### Data Warehouse in R

![](images/rdata.png)

<!--chapter:end:1_introMD.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Pre procesamiento

  - Recopilación
  - Importación
  - Exploración
    * Diccionario de variables
    * Niveles de agregación
    * Descripción univariada 
    * Identificando relaciones
    * Aproximación Visual (Visualización)
  - Filtrado y selección
    * Filtrado de observaciones
    * Selección de variables
    * Pivot, Reshape
    * Uniendo bases de datos
  - Muestreo, estimación, error estándar y confiabilidad
    * Diseño muestral
    * Estimación
    * Rendimiento
  - Transformación
    * Adecuación de formatos
    * Limpieza de texto
    * Creación de variables
    * Valores atípicos
    * Valores perdidos

## Importación

```{r}
#bookdown::clean_book(TRUE)
#bookdown::render_book("index.Rmd", "bookdown::gitbook")
rm(list=ls())
load(url("https://github.com/AlvaroLimber/EST-383/raw/master/data/oct20.RData"))
```

## Exploración

  + Filas/observaciones: Mesas electorales
  + Hay información a nivel de mesas según el tipo de elección.
  + La cobertura espacial: No solo es Bolivia

### Diccionario de variables

```{r}
dim(computo)
nrow(computo)
ncol(computo)
names(computo)
str(computo)
str(computo$País)
typeof(computo$País)
```

### Agregación/Niveles

```{r}
aux<-unique(computo$Elección)
length(unique(computo$`Código Mesa`))
library(dplyr)
#filtrado ctr+alt*M
bdpv<-computo %>% filter(Elección==aux[1])
bddu<-computo %>% filter(Elección==aux[2])
bdde<-computo %>% filter(Elección==aux[3])
save(bdpv,bddu,bdde,file="bd_elecciones20.RData")
```

### Descripción univariada

  + Tablas de frecuencia (cualitativas)
  + Reportes estadísticos; media, sd, etc. (Cuantitativas)
  + Gráficos (mix)

```{r}
# Ejercicio: Crear un vector que haga la diferencia entre variables cualitativas y cuantitativas.
dcuanti<-function(bd){
  bd<-data.frame(bd)
  aux<-NULL
  for(i in 1:ncol(bd)){
    aux[i]<-is.numeric(bd[,i])
  }
  return(aux)
}
dcuanti(bdpv)

#typeof(bdpv)
#str(bdpv)

#bdpv<-data.frame(bdpv)
#is.numeric(bdpv[,"MNR"])
#is.numeric(bdpv[,21])
#is.numeric(bdpv$MNR)
vcuanti<-names(bdpv)[dcuanti(bdpv)]
vcuali<-names(bdpv)[!dcuanti(bdpv)]
library(Hmisc)
describe(bdpv[,vcuanti])
describe(bdpv[,vcuali])
#Tarea: Escribir una función que permita explorar las características de las variables, según su naturaleza
#cualitativa
t1<-table(bdpv$Departamento)
aux1<-as.data.frame(prop.table(t1)*100)#%
aux2<-as.data.frame(t1)#frecuencias
names(aux1)[2]<-"Porcentaje"
tt1<-merge(aux1,aux2)#unir base de datos
head(tt1)
library(dplyr)#Gramática de datos

bdpv %>% group_by(Departamento) %>% count() 

tt1<-bdpv %>% group_by(Departamento) %>% summarise(Freq=n()) %>% mutate(Porcentaje=(Freq/sum(Freq))*100)
library(ggplot2)# gramàtica de gràficos
ggplot(bdpv,aes(Departamento))+geom_bar()

tt1

ggplot(tt1[1:10,],aes(Departamento,Porcentaje))+geom_bar(stat="identity")

ggplot(tt1 %>% filter(Porcentaje>1),aes(Departamento,Porcentaje))+geom_bar(stat="identity")

#cuantitativas
mean(bdpv$Blancos)
median(bdpv$Blancos)
summary(bdpv$Blancos)
sd(bdpv$Blancos)
sd(bdpv$Blancos)/mean(bdpv$Blancos)
quantile(bdpv$Blancos,seq(0,1,0.1))
quantile(bdpv$Blancos,seq(0,1,0.05))
hist(bdpv$Blancos)
plot(density(bdpv$Blancos))
boxplot(bdpv$Blancos)

boxplot(log(bdpv$Blancos))
plot(density(log(bdpv$Blancos)))
```

## Relaciones entre 2 variables 
  * Cuali vs cuali
  * Cuanti vs cuanti
  * Cuali vs cuanti
  
```{r}
rm(list=ls())
load(url("https://github.com/AlvaroLimber/EST-384/raw/master/data/eh19.RData"))
#cuali vs cuali
#eh19p$s02a_02#sexo
#eh19p$p0#pobreza monetaria moderada

t2<-table(eh19p$s02a_02,eh19p$p0)
t2
prop.table(t2)
prop.table(t2,1)
prop.table(t2,2)
chisq.test(t2)# H0: Independencia
plot(eh19p$s02a_02,eh19p$p0)
plot(eh19p$area,eh19p$p0)
plot(eh19p$niv_ed,eh19p$p0)

chisq.test(table(eh19p$niv_ed,eh19p$p0))
```

### Cuanti vs Cuanti

```{r}
#eh19p$s02a_03#edad
#eh19p$aestudio# años de educación
#eh19p$ylab# ingreso laboral

cor(eh19p[,c("s02a_03","aestudio","ylab")],use="complete.obs")
cor(eh19p[,c("s02a_03","aestudio","ylab")],use="pairwise.complete.obs")

nrow(na.omit(eh19p[,c("s02a_03","aestudio","ylab")]))
nrow(na.omit(eh19p[,c("s02a_03","aestudio")]))

library(GGally)

ggpairs(eh19p,columns = c("s02a_03","aestudio","ylab"))

ggpairs(eh19p,columns = c("s02a_02","aestudio","ylab","p0"))
```

## Filtrado y selección

Es el proceso que reduce la base de datos a la **población objetivo** y especifíca las **variables** a utilizar en el **modelado** posterior. Además de incorporar otras variables provenientes de otras bases de datos si es necesario.

Por ejemplo:

  * Para los resultados electorales de 2020 presidente y vicepresidente: Se quiere estudiar el comportamiento de votos Nulos y Blancos para el departamento de Tarija
  * Para la encuesta a hogares 2019, se quiere estudiar los años de educación, condición de pobreza moderada, edad e ingreso laboral de las jefas de hogar.

### Selección/filtrado de observaciones (filas)

El objetivo en este punto es trabajar sobre la unidad de análisis, que define a la población objetivo.

Ejercicio: obtener la base de datos filtrada de los ejemplos anteriores. 

```{r}
rm(list = ls())
library(dplyr)
#electoral
load(url("https://github.com/AlvaroLimber/MD_umsa/raw/main/DataMining/bd_elecciones20.RData"))
unique(bdpv$Departamento)
bdtj<-bdpv %>% filter(Departamento=="Tarija")
#eh 2019
load(url("https://github.com/AlvaroLimber/EST-384/raw/master/data/eh19.RData"))

aux<-unique(eh19p$s02a_02)
aux[2]
relacion<-unique(eh19p$s02a_05)

ehjm<-eh19p %>% filter(s02a_02==aux[2] & s02a_05==relacion[1])
```

Ejercicio

Obtener una base de datos de la EH19 que incluya solamente a los suegros, padres y otros parientes.

```{r}
relacion[c(4,8,10)]
ehmis<-eh19p %>% filter(s02a_05 %in% relacion[c(4,8,10)])
```

Ejemplo, se quiere estudiar la bd electoral para presidente en todas las mesas, excepto las de Tarija.

```{r}
bdntj<-bdpv %>% filter(Departamento!="Tarija")
```

### Selección de variables (columnas)

El objetivo es trabajar únicamente con las variables necesarias para el estudio, es decir, las que se utilizaran en el modelado posterior.

Ejemplo, realizar la selección de variables para los ejemplos anteriores:

```{r}
# Para los resultados electorales de 2020 presidente y vicepresidente: Se quiere estudiar el comportamiento de votos Nulos y Blancos para el departamento de Tarija
names(bdpv)
bdtj<-bdpv %>% filter(Departamento=="Tarija") %>% select(Blancos,Nulos,`Votos Válidos`,Provincia,Municipio,Recinto)
object.size(bdpv)
object.size(bdtj)

#Para la encuesta a hogares 2019, se quiere estudiar los años de educación, condición de pobreza moderada, edad e ingreso laboral de las jefas de hogar.
ateh<-attributes(eh19p)
ehjm<-eh19p %>% filter(s02a_02==aux[2] & s02a_05==relacion[1]) %>% select(aestudio,p0,s02a_03,ylab,folio,nro,factor,estrato, upm)
#eh19p %>% select(-folio)
#eh19p %>% select(-c(folio,area))
#eh19p %>% select(1:5) # primeras 5 variables
#eh19p %>% select(z:pext2)  
```

### Pivot, Reshape

```{r}
rm(list=ls())
load(url("https://github.com/AlvaroLimber/MD_umsa/raw/main/data/eh21.RData"))
```

> Problema: Cuál es la cantidad promedio de tenencia del equipamiento (17 items) en los hogares?. Mostrar un histograma.

```{r}
library(dplyr)
library(haven)
bdeq<-eh21eq %>% mutate(aux=(s09c_14==1)) %>% group_by(folio) %>%summarise(equip=sum(aux))
mean(bdeq$equip)
hist(bdeq$equip)
```

> Problema: Generar una base de datos a nivel de folio, donde existan 17 variables que indiquen si el hogar cuenta o no con un determinado equipamiento.

```{r}
#PIVOT RESHAPE
library(tidyr)
#?pivot_longer()
#?pivot_wider()
pv_w<-eh21eq %>% select(folio,item,s09c_14) %>% pivot_wider(names_from = item, values_from = s09c_14,names_prefix = "it.")
head(pv_w)
```

Ahora usemos más de una variable en el values_from

```{r}
ej<-eh21eq %>% select(folio,item,s09c_14:s09c_15) %>% pivot_wider(names_from = item, values_from = s09c_14:s09c_15)

ej2<-eh21eq %>% select(folio,item,s09c_14:s09c_17) %>% pivot_wider(names_from = item, values_from = s09c_14:s09c_17)
```

Tarea para su casa: Usar el pivot_longer

### Uniendo bases de datos

Vamos a distinguir 2 tipos de uniones:

  * Con la finalidad de incrementar observaciones (filas)
  * Con la finalidad de incrementar variables (columnas)

> Problema: Crear 2 bases de datos a partir de la base de viviendas, según el departamento: Una para Beni, la otra Cochabamba. Luego unir esas bases de datos.

  
```{r}
bdcb<-eh21v %>% filter(depto==3)
bdbn<-eh21v %>% filter(depto==8)
rbind(bdcb,bdbn)#Base R
bind_rows(bdcb,bdbn)# dplyr
```

Recomendaciones:

  * Se debe adecuar los *nombres* de las variables en las bases de datos que se van a unir. Asegurarse que sean los mismos nombres, verificar las mayúsculas, minúsculas.
  * Asegurarse que las variables similares en las bases de datos tengan el mismo *formato*.

```{r}
str(bdcb$depto)
str(bdbn$depto)
str(bdcb$estrato)
aux<-bind_rows(bdcb %>% mutate(estrato=as.numeric(estrato)),bdbn)
```

> Problema: Unir la base de datos de equipamiento ej2 con la base de datos de Beni generada en el problema anterior.

```{r}
aux<-merge(bdbn,ej2)
aux<-merge(ej2,bdbn)
aux<-merge(ej2,bdbn,all.x = T)
auxl<-left_join(bdbn,ej2)
auxr<-right_join(bdbn,ej2)
aux<-inner_join(bdbn,ej2)

auxl<-bdbn %>% left_join(ej2) %>% left_join(eh21di)
auxr<-bdbn %>% right_join(ej2)
```

## Muestreo, estimación, error estándar y confiabilidad

  * Si el objetivo del estudio es realizar un proceso de inferencia, es decir, usar los resultados de una muestra para generalizar a la población. Es importante definir el diseño muestral. 
  * Esto no es necesario si el objetivo no es generalizar los resultados.
  
### Estimador de Horvitz Thomson  

Este estimador es un estimador genérico y nos permite identificar los elementos necesarios para realizar estimaciones a partir de una muestra. 

Recordemos el parámetro del total.

$$t_y=\sum_U y_k$$

$$\mu_y=\frac{t_y}{N}$$  
El estimador de HT del total es:

$$\hat{t}_y=\sum_s\frac{y_k}{\pi_k}=\sum_s y_k w_k$$
  
Donde $w_k=\pi_k^{-1}$, $w_k$ se conoce como el factor de expansión, que se encuentra en términos de la probabilidad de selección.

> Nota: Para lograr las estimaciones de muestreo, únicamente es necesario contar con el factor de expansión.

$$V(\hat{t}_y)=\sum_U\sum_U \frac{y_k}{\pi_k}\frac{y_l}{\pi_l} (\pi_{kl}-\pi_k \pi_l) $$

$$\hat{V}(\hat{t}_y)=\sum_s\sum_s \frac{y_k}{\pi_k}\frac{y_l}{\pi_l} \frac{(\pi_{kl}-\pi_k \pi_l)}{\pi_{kl}} $$

Para conocer el error de un estimador, es necesario conocer el diseño muestral, ya que este permite construir la varianza del estimador. Las medidas más usuales para conocer el error de un estimador son:

$$\text{Error estándar}=EE(\hat{\theta})=\sqrt{V(\hat{\theta)}}$$
$$\text{Error relativo}=cv(\hat{\theta})=\frac{EE(\hat{\theta})}{\hat{\theta}}$$
Este $cv$ es como un semáforo, recordar que el objetivo de un estimador es aproximar de la mejor forma posible al parámetro.

  * cv es menor 0.05 es un muy buen estimador, es útil para tomar alguna acciones
  * si el cv se encuentra entre 0.05 y 0.1 es aceptable
  * si el cv se encuentra entre 0.1 y 0.25, usar solo con fines descriptivos/informativas
  * si el cv supera 0.25 el estimador es malo, no debe usarse.

> Nota: Al momento de desagregar la información, es importante asegurarse que exista la muestra suficiente en las celdas que genera la desagregación. Esta muestra puede ser mayor a 20, 50. También es recomendable analizar el efecto de diseño.

$$Deff=\frac{V_{complejo}}{V_{mas}}$$

### Diseño muestral

Al momento de trabajar con una encuesta, es importante identificar:

  * Factores de expansión (estimación)
  * Características del diseño muestral (error del estimador)
    - Etapas (cluster/conglomerados)
    - Estratificación; en que etapas
    - Factores por finitud; Ejemplo $n/N$

```{r}
library(survey)# Más antigua
library(srvyr)# trabaja con la dplyr 
load(url("https://github.com/AlvaroLimber/MD_umsa/raw/main/data/eh21.RData"))
# Factor de expansión: factor
# Conglomerado: UPM
# Estratificación: estrato
eh21p %>% group_by(estrato) %>% summarise(n())

eh21p %>% group_by(upm) %>% summarise(nper=n()) %>% summarise(mean(nper),max(nper),min(nper))
length(unique(eh21p$upm))
#factor
summary(eh21p$factor)
library(ggplot2)
ggplot(eh21p,aes(factor))+geom_density()
sum(eh21p$factor)
eh21p$factor
quantile(eh21p$factor,c(0.99))
eh21p %>% filter(factor>1306) %>% group_by(area,depto,estrato,p0) %>% summarise(n())
```

### Estimación

### Rendimiento

## Transformación
### Adecuación de formatos
### Limpieza de texto
### Creación de variables
### Valores atípicos
### Valores perdidos

<!--chapter:end:2_preprocesamiento.Rmd-->

